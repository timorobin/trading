{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# following the example here: \n",
    "- https://github.com/patrick-kidger/torchcde/blob/master/example/example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchcde\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CDE model looks like\n",
    "\n",
    "$ z_t = z_0 + \\int_0^t f_\\theta(z_s) dX_s $\n",
    "\n",
    "Where $X$ is your data and $f_\\theta$ is a neural network. So the first thing we need to do is define such an $f_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what this CDEFunc class does.\n",
    "Here we've built a small single-hidden-layer neural network, whose hidden layer is of width 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDEFunc(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        ######################\n",
    "        # input_channels is the number of input channels in the data X. (Determined by the data.)\n",
    "        # hidden_channels is the number of channels for z_t. (Determined by you!)\n",
    "        ######################\n",
    "        super(CDEFunc, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, input_channels * hidden_channels)\n",
    "        \n",
    "    ######################\n",
    "    # For most purposes the t argument can probably be ignored; unless you want your CDE to behave differently at\n",
    "    # different times, which would be unusual. But it's there if you need it!\n",
    "    ######################\n",
    "    def forward(self, t, z):\n",
    "        # z has shape (batch, hidden_channels)\n",
    "        z = self.linear1(z)\n",
    "        z = z.relu()\n",
    "        z = self.linear2(z)\n",
    "        ######################\n",
    "        # Easy-to-forget gotcha: Best results tend to be obtained by adding a final tanh nonlinearity.\n",
    "        ######################\n",
    "        z = z.tanh()\n",
    "        ######################\n",
    "        # Ignoring the batch dimension, the shape of the output tensor must be a matrix,\n",
    "        # because we need it to represent a linear map from R^input_channels to R^hidden_channels.\n",
    "        ######################\n",
    "        z = z.view(z.size(0), self.hidden_channels, self.input_channels)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to package CDEFunc up into a model that computes the integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels):\n",
    "        super(NeuralCDE, self).__init__()\n",
    "\n",
    "        self.func = CDEFunc(input_channels, hidden_channels)\n",
    "        self.initial = torch.nn.Linear(input_channels, hidden_channels)\n",
    "        self.readout = torch.nn.Linear(hidden_channels, output_channels)\n",
    "\n",
    "    def forward(self, coeffs):\n",
    "        X = torchcde.NaturalCubicSpline(coeffs)\n",
    "\n",
    "        ######################\n",
    "        # Easy to forget gotcha: Initial hidden state should be a function of the first observation.\n",
    "        ######################\n",
    "        X0 = X.evaluate(X.interval[0])\n",
    "        z0 = self.initial(X0)\n",
    "\n",
    "        ######################\n",
    "        # Actually solve the CDE.\n",
    "        ######################\n",
    "        z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.func,\n",
    "                              t=X.interval)\n",
    "\n",
    "        ######################\n",
    "        # Both the initial value and the terminal value are returned from cdeint; extract just the terminal value,\n",
    "        # and then apply a linear map.\n",
    "        ######################\n",
    "        z_T = z_T[:, 1]\n",
    "        pred_y = self.readout(z_T)\n",
    "        return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Now we need some data.\n",
    "# Here we have a simple example which generates some spirals, some going clockwise, some going anticlockwise.\n",
    "######################\n",
    "def get_data():\n",
    "    t = torch.linspace(0., 4 * math.pi, 100)\n",
    "\n",
    "    start = torch.rand(128) * 2 * math.pi\n",
    "    x_pos = torch.cos(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos[:64] *= -1\n",
    "    y_pos = torch.sin(start.unsqueeze(1) + t.unsqueeze(0)) / (1 + 0.5 * t)\n",
    "    x_pos += 0.01 * torch.randn_like(x_pos)\n",
    "    y_pos += 0.01 * torch.randn_like(y_pos)\n",
    "    ######################\n",
    "    # Easy to forget gotcha: time should be included as a channel; Neural CDEs need to be explicitly told the\n",
    "    # rate at which time passes. Here, we have a regularly sampled dataset, so appending time is pretty simple.\n",
    "    ######################\n",
    "    X = torch.stack([t.unsqueeze(0).repeat(128, 1), x_pos, y_pos], dim=2)\n",
    "    y = torch.zeros(128)\n",
    "    y[:64] = 1\n",
    "\n",
    "    perm = torch.randperm(128)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "\n",
    "    ######################\n",
    "    # X is a tensor of observations, of shape (batch=128, sequence=100, channels=3)\n",
    "    # y is a tensor of labels, of shape (batch=128,), either 0 or 1 corresponding to anticlockwise or clockwise respectively.\n",
    "    ######################\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# input_channels=3 because we have both the horizontal and vertical position of a point in the spiral, and time.\n",
    "# hidden_channels=8 is the number of hidden channels for the evolving z_t, which we get to choose.\n",
    "# output_channels=1 because we're doing binary classification.\n",
    "######################\n",
    "model = NeuralCDE(input_channels=3, hidden_channels=8, output_channels=1)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Now we turn our dataset into a continuous path. We do this here via natural cubic spline interpolation.\n",
    "# The resulting `train_coeffs` is a tensor describing the path.\n",
    "# For most problems, it's probably easiest to save this tensor and treat it as the dataset.\n",
    "######################\n",
    "train_coeffs = torchcde.natural_cubic_coeffs(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Training loss: 0.9556078314781189\n",
      "Epoch: 1   Training loss: 1.0249571800231934\n",
      "Epoch: 2   Training loss: 0.6855239272117615\n",
      "Epoch: 3   Training loss: 0.8307387828826904\n",
      "Epoch: 4   Training loss: 0.745247483253479\n",
      "Epoch: 5   Training loss: 0.6234510540962219\n",
      "Epoch: 6   Training loss: 0.6001538038253784\n",
      "Epoch: 7   Training loss: 0.5706719160079956\n",
      "Epoch: 8   Training loss: 0.5286034941673279\n",
      "Epoch: 9   Training loss: 0.4988443851470947\n",
      "Epoch: 10   Training loss: 0.43296709656715393\n",
      "Epoch: 11   Training loss: 0.3257259726524353\n",
      "Epoch: 12   Training loss: 0.21124131977558136\n",
      "Epoch: 13   Training loss: 0.12150489538908005\n",
      "Epoch: 14   Training loss: 0.05994701758027077\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "train_dataset = torch.utils.data.TensorDataset(train_coeffs, train_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch_coeffs, batch_y = batch\n",
    "        pred_y = model(batch_coeffs).squeeze(-1)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print('Epoch: {}   Training loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "test_X, test_y = get_data()\n",
    "test_coeffs = torchcde.natural_cubic_coeffs(test_X)\n",
    "pred_y = model(test_coeffs).squeeze(-1)\n",
    "binary_prediction = (torch.sigmoid(pred_y) > 0.5).to(test_y.dtype)\n",
    "prediction_matches = (binary_prediction == test_y).to(test_y.dtype)\n",
    "proportion_correct = prediction_matches.sum() / test_y.size(0)\n",
    "print('Test Accuracy: {}'.format(proportion_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.8642,  4.7660, -3.3683, -3.4361,  4.8082, -3.0356,  4.6428, -3.1727,\n",
       "         2.8118,  3.6808, -3.1978, -3.2700, -3.1142,  4.8418, -3.4221,  2.9090,\n",
       "        -2.3949,  2.2152,  3.4612,  2.7512, -3.0920,  2.4418, -3.1267, -3.2749,\n",
       "         4.4941, -3.3270,  2.7463, -2.9806,  4.4395, -2.5967,  2.7151,  2.7531,\n",
       "         2.7058, -3.0503, -3.0278, -2.4734, -3.6836,  4.8273,  2.4549, -2.4488,\n",
       "        -3.0870, -2.4042,  3.4958, -2.4246,  2.7067,  4.5567, -2.4310, -2.9452,\n",
       "         2.9359,  3.8290, -3.6689,  2.7041, -2.9999, -3.7177, -3.6549,  2.7057,\n",
       "        -2.4493,  3.4272, -3.0260, -3.6310,  2.5254,  4.8286,  4.1304, -2.4858,\n",
       "        -2.4195, -2.7480,  4.4479, -3.0528,  3.4386,  2.5928,  2.3664,  3.6087,\n",
       "         2.8040, -3.2621,  4.6705,  2.7597,  2.7862,  2.8325, -2.5907, -2.9079,\n",
       "        -2.6772,  2.7817,  4.5088, -3.1706,  4.5548,  2.7352,  4.6248, -3.5122,\n",
       "         4.6695,  2.8016, -3.5389, -2.6833,  3.9014, -3.0424, -2.3798,  2.8218,\n",
       "         3.6908, -3.3098, -3.2880, -3.3477,  4.7908,  2.7050, -3.0960, -3.2473,\n",
       "        -3.2231,  2.6825, -2.3750, -3.6078,  3.2684,  4.1969, -2.5412,  2.8066,\n",
       "        -3.0780,  2.7280, -2.7351,  2.7362, -2.3927, -3.2691,  4.8423, -3.0249,\n",
       "        -3.0704,  2.7510, -2.4519,  4.8423,  2.6246, -2.4838,  2.7076,  4.7232],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
